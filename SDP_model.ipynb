{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# A Stochastic Dynamic Programming Model for Post-Disaster Relief Delivery considering the Deprivation Level"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 0. Install packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Library for processing, analyzing, and displaying data\n",
        "import pandas as pd\n",
        "\n",
        "# Library for dictionary subclass that remembers the order in which keys were first inserted\n",
        "from collections import OrderedDict\n",
        "\n",
        "# Library for the creation, manipulation, and study of complex networks of nodes and edges\n",
        "import networkx as nx\n",
        "\n",
        "# Library used for creating static, interactive, and animated visualizations\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Library for calculus\n",
        "import numpy as np\n",
        "\n",
        "# Library for finding a solution to the system of equations\n",
        "from scipy.optimize import fsolve\n",
        "\n",
        "# Library for data visualization\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Read arc data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_data(file_path: str):\n",
        "    df = pd.read_excel(file_path)\n",
        "    df = df.astype({\n",
        "        \"from_node\": int,\n",
        "        \"to_node\": int,\n",
        "        \"t_planned\": float,\n",
        "        \"t_delayed\": float,\n",
        "        \"p_planned\": float,\n",
        "        \"p_delayed\": float\n",
        "    })\n",
        "    return df\n",
        "\n",
        "file_path = \"/Users/minhthipham/Library/CloudStorage/OneDrive-ErasmusUniversityRotterdam/Thesis/04_Simulation network/Arc data.xlsx\"\n",
        "df = load_data(file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Create a graph and map paths to arcs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a directed graph\n",
        "G = nx.DiGraph()\n",
        "\n",
        "# Add edges (arcs) to the graph\n",
        "G.add_edges_from([(0, 1), (0, 2), (0, 3),\n",
        "                  (1, 4), (1, 5), (1, 6),\n",
        "                  (2, 4), (2, 5), (2, 6),\n",
        "                  (3, 4), (3, 5), (3, 6),\n",
        "                  (4, 7), (4, 8),\n",
        "                  (5, 7), (5, 8),\n",
        "                  (6, 7), (6, 8),\n",
        "                  (7, 9), \n",
        "                  (8, 9)])\n",
        "\n",
        "# Define node positions\n",
        "positions = {0: (0, 1), 1: (1, 2), 2: (1, 1), 3: (1, 0),\n",
        "       4: (2, 2), 5: (2, 1), 6: (2, 0),\n",
        "       7: (3, 1.5), 8: (3, 0.5),\n",
        "       9: (4, 1)}\n",
        "\n",
        "# Plot the graph\n",
        "nx.draw(G, positions, with_labels=True, node_size=1500, node_color='lightgrey', font_weight='bold', arrowsize=20)\n",
        "plt.show()\n",
        "\n",
        "# Find all paths from source to target\n",
        "all_paths = list(nx.all_simple_paths(G, source=0, target=max(G.nodes)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Nodes and their descendants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reverse the graph to calculate predecessors\n",
        "G_reversed = G.reverse()\n",
        "descendants_dict = OrderedDict((node, list(G_reversed.predecessors(node))) for node in G_reversed.nodes())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. All possible states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Precompute deprivation levels for a relevant range of times\n",
        "unique_times = set()\n",
        "def calculate_states(df):\n",
        "    for _, row in df.iterrows():\n",
        "        from_node = int(row['from_node'])\n",
        "        to_node = int(row['to_node'])\n",
        "        t_planned = row['t_planned']\n",
        "        t_delayed = row['t_delayed']\n",
        "\n",
        "        if from_node in states_dict:\n",
        "            if to_node not in states_dict:\n",
        "                states_dict[to_node] = []\n",
        "            for state in states_dict[from_node]:\n",
        "                current_time = state[1]\n",
        "                new_time_planned = round(current_time + t_planned, 4)\n",
        "                new_time_delayed = round(current_time + t_delayed, 4)\n",
        "                states_dict[to_node].append((to_node, new_time_planned))\n",
        "                states_dict[to_node].append((to_node, new_time_delayed))\n",
        "                unique_times.update([new_time_planned, new_time_delayed])\n",
        "\n",
        "states_dict = OrderedDict([(0, [(0, 0)])]) # Initialize states_dict\n",
        "calculate_states(df)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. Define Deprivation Level Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the deprivation level function\n",
        "def deprivation_level_function(x):\n",
        "    return 9.772697 / (1 + 3.9031 * np.exp(-0.7919 * x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6. Inflection point"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the original function and its derivatives\n",
        "def Y(X):\n",
        "    return 9.772697 / (1 + 3.9031 * np.exp(-0.7919 * X))\n",
        "\n",
        "def dY_dX(X):\n",
        "    A = 9.772697\n",
        "    B = 3.9031\n",
        "    k = 0.7919\n",
        "    return (A * k * B * np.exp(-k * X)) / ((1 + B * np.exp(-k * X)) ** 2)\n",
        "\n",
        "def d2Y_dX2(X):\n",
        "    A = 9.772697\n",
        "    B = 3.9031\n",
        "    k = 0.7919\n",
        "    term1 = (-A * k**2 * B * np.exp(-k * X)) / ((1 + B * np.exp(-k * X)) ** 2)\n",
        "    term2 = (2 * A * k**2 * B**2 * np.exp(-2 * k * X)) / ((1 + B * np.exp(-k * X)) ** 3)\n",
        "    return term1 + term2\n",
        "\n",
        "# Generate X values\n",
        "X_values = np.linspace(0, 10, 400)\n",
        "\n",
        "# Calculate Y, first derivative, and second derivative values\n",
        "Y_values = Y(X_values)\n",
        "dY_dX_values = dY_dX(X_values)\n",
        "d2Y_dX2_values = d2Y_dX2(X_values)\n",
        "\n",
        "# Find the X value where the first derivative has its maximum\n",
        "max_dY_dX_index = np.argmax(dY_dX_values)\n",
        "max_dY_dX_X_value = X_values[max_dY_dX_index]\n",
        "\n",
        "# Find the inflection point (where second derivative equals zero)\n",
        "inflection_point = fsolve(d2Y_dX2, 2)[0]\n",
        "\n",
        "# Set font to Times New Roman\n",
        "plt.rcParams['font.family'] = 'Times New Roman'\n",
        "\n",
        "# Plot the original function\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "plt.subplot(3, 1, 1)\n",
        "plt.plot(X_values, Y_values, label='Y(X)', color='black')\n",
        "plt.axvline(x=inflection_point, color='#5A6F97', linestyle='--', label='Inflection Point')\n",
        "plt.title('Deprivation Level over Time')\n",
        "plt.xlabel('Deprivation Time (days)')\n",
        "plt.ylabel('Deprivation Level')\n",
        "plt.xlim(0, 8)\n",
        "plt.grid(True, which='both', linewidth=0.5, color='lightgrey')\n",
        "plt.legend()\n",
        "\n",
        "# Plot the first derivative\n",
        "plt.subplot(3, 1, 2)\n",
        "plt.plot(X_values, dY_dX_values, label=\"dY/dX\", color='black')\n",
        "plt.axvline(x=inflection_point, color='#5A6F97', linestyle='--', label='Inflection Point')\n",
        "plt.title('Rate of Change of Deprivation Level')\n",
        "plt.xlabel('Deprivation Time (days)')\n",
        "plt.ylabel('dY/dX')\n",
        "plt.xlim(0, 8)\n",
        "plt.grid(True, which='both', linewidth=0.5, color='lightgrey')\n",
        "plt.legend()\n",
        "\n",
        "# Plot the second derivative\n",
        "plt.subplot(3, 1, 3)\n",
        "plt.plot(X_values, d2Y_dX2_values, label=\"d2Y/dX2\", color='black')\n",
        "plt.axvline(x=inflection_point, color='#5A6F97', linestyle='--', label='Inflection Point')\n",
        "plt.title('Acceleration of Change of Deprivation Level')\n",
        "plt.xlabel('Deprivation Time (days)')\n",
        "plt.ylabel('d2Y/dX2')\n",
        "plt.xlim(0, 8)\n",
        "plt.grid(True, which='both', linewidth=0.5, color='lightgrey')\n",
        "plt.legend()\n",
        "\n",
        "# Adjust the space between plots\n",
        "plt.subplots_adjust(hspace=0.5)  # Adjust the hspace value for better spacing\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('Derivatives_DLF.pdf', format='pdf', bbox_inches='tight', dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7. Recursion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper function to get transition probabilities and times\n",
        "def get_transition_values(n, d):\n",
        "    row = df.loc[(df[\"from_node\"] == n) & (df[\"to_node\"] == d)]\n",
        "    p_delayed = row[\"p_delayed\"].values[0]\n",
        "    p_planned = row[\"p_planned\"].values[0]\n",
        "    t_delayed = row[\"t_delayed\"].values[0]\n",
        "    t_planned = row[\"t_planned\"].values[0]\n",
        "    return p_planned, p_delayed, t_planned, t_delayed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Determine the optimal policy\n",
        "policy = {}\n",
        "v_dict = OrderedDict()\n",
        "v_dict_all = OrderedDict()\n",
        "final_node = max(G.nodes)\n",
        "\n",
        "for n, states in reversed(states_dict.items()):\n",
        "    for state in states:\n",
        "        tau = state[1]\n",
        "        if n == final_node:\n",
        "            v_dict[state] = deprivation_level_function(tau)\n",
        "        else:\n",
        "            min_V = np.inf\n",
        "            best_d = None\n",
        "            for d in descendants_dict.get(n, []):\n",
        "                p_planned, p_delayed, t_planned, t_delayed = get_transition_values(n, d)\n",
        "                v_d_planned = v_dict.get((d, round(tau + t_planned, 4)))\n",
        "                v_d_delayed = v_dict.get((d, round(tau + t_delayed, 4)))\n",
        "                V = p_planned * v_d_planned + p_delayed * v_d_delayed\n",
        "                v_dict_all[(n, tau, d)] = V\n",
        "                if V < min_V:\n",
        "                    min_V = V\n",
        "                    best_d = d\n",
        "            # cache deprivation levels\n",
        "            v_dict[state] = min_V\n",
        "            if best_d is not None:\n",
        "                policy[state] = {\n",
        "                    \"best_next_node\": best_d, \n",
        "                    \"total_expected_deprivation_level\": min_V\n",
        "                }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8. Final policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import copy\n",
        "\n",
        "# Determine the optimal policy\n",
        "policy_final = copy.deepcopy(policy)\n",
        "for state, _ in policy_final.items():\n",
        "    n = state[0]\n",
        "    tau = state[1]\n",
        "    min_t_planned = np.inf\n",
        "    if tau >= inflection_point:\n",
        "        for d in descendants_dict.get(n, []):\n",
        "            p_planned, _, t_planned, _ = get_transition_values(n, d)\n",
        "            if t_planned < min_t_planned:\n",
        "                min_t_planned = t_planned\n",
        "                min_p_planned = p_planned\n",
        "                best_d = d\n",
        "            elif t_planned == min_t_planned and p_planned > min_p_planned: # if any descendants have the same t_planned choose the one with the higher probability\n",
        "                min_t_planned = t_planned\n",
        "                min_p_planned = p_planned\n",
        "                best_d = d\n",
        "        policy_final[state][\"best_next_node\"] = best_d\n",
        "        policy_final[state][\"total_expected_deprivation_level\"] = v_dict_all[n, tau, best_d]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "policy_df = pd.DataFrame.from_dict(data=policy_final, orient='index')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Transforming the dictionary into a DataFrame\n",
        "data = []\n",
        "for key, value in policy_final.items():\n",
        "    row = {\n",
        "        'state': key,\n",
        "        'best_next_node': value['best_next_node'],\n",
        "        'total_expected_deprivation_level': value['total_expected_deprivation_level']\n",
        "    }\n",
        "    data.append(row)\n",
        "\n",
        "df_out = pd.DataFrame(data)\n",
        "\n",
        "# Save the DataFrame to an Excel file\n",
        "df_out.to_excel('output.xlsx', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9. Expected Shortest Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a directed graph\n",
        "G_2 = nx.DiGraph()\n",
        "\n",
        "# Add edges with expected travel time as weights\n",
        "for _, row in df.iterrows():\n",
        "    expected_travel_time = row['t_planned'] * row['p_planned'] + row['t_delayed'] * row['p_delayed']\n",
        "    G_2.add_edge(row['from_node'], row['to_node'], weight=expected_travel_time)\n",
        "\n",
        "# Compute the shortest path based on expected travel time\n",
        "source = 0\n",
        "target = max(G_2.nodes)\n",
        "shortest_path = nx.shortest_path(G_2, source=source, target=target, weight='weight')\n",
        "shortest_path_length = nx.shortest_path_length(G_2, source=source, target=target, weight='weight')\n",
        "shortest_path_DL = deprivation_level_function(shortest_path_length)\n",
        "\n",
        "# Display results\n",
        "print(f\"The shortest path from node {source} to node {target} is: {shortest_path}\")\n",
        "print(f\"The total expected travel time is: {shortest_path_length} days\")\n",
        "print(f\"The total expected deprivation level is: {shortest_path_DL}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a directed graph\n",
        "G_2 = nx.DiGraph()\n",
        "\n",
        "# Add edges with expected travel time as weights\n",
        "for _, row in df.iterrows():\n",
        "    expected_travel_time = round(row['t_planned'] * row['p_planned'] + row['t_delayed'] * row['p_delayed'], 4)\n",
        "    G_2.add_edge(row['from_node'], row['to_node'], weight=expected_travel_time)\n",
        "\n",
        "# Define source and target nodes for the shortest path calculation\n",
        "source_node = 0\n",
        "target_node = max(G_2.nodes)\n",
        "\n",
        "# Apply Dijkstra's algorithm to find the shortest path and its length\n",
        "shortest_path = nx.dijkstra_path(G_2, source=source_node, target=target_node)\n",
        "shortest_path_length = nx.dijkstra_path_length(G_2, source=source_node, target=target_node)\n",
        "\n",
        "# Initialize the table with infinity distances\n",
        "nodes = list(G_2.nodes)\n",
        "table = pd.DataFrame(index=nodes, columns=['Distance', 'Previous Node'])\n",
        "table['Distance'] = float('inf')\n",
        "table['Previous Node'] = None\n",
        "\n",
        "# Function to update the table\n",
        "def update_table(node, distance, previous_node):\n",
        "    table.at[node, 'Distance'] = distance\n",
        "    table.at[node, 'Previous Node'] = previous_node\n",
        "\n",
        "# Initialize the source node\n",
        "update_table(source_node, 0, '-')\n",
        "\n",
        "# Dijkstra's algorithm steps\n",
        "steps_table_columns = ['Step', 'Current Node', 'Distance from Source', 'Neighbors', 'Distance Update', 'Updated Node Distances', 'Unvisited Nodes']\n",
        "steps_table = pd.DataFrame(columns=steps_table_columns)\n",
        "\n",
        "unvisited_nodes = set(nodes)\n",
        "current_node = source_node\n",
        "step = 1\n",
        "\n",
        "while unvisited_nodes:\n",
        "    current_distance = table.loc[current_node, 'Distance']\n",
        "    neighbors = list(G_2.neighbors(current_node))\n",
        "    distance_update = {}\n",
        "    for neighbor in neighbors:\n",
        "        if neighbor in unvisited_nodes:\n",
        "            edge_weight = G_2[current_node][neighbor]['weight']\n",
        "            new_distance = current_distance + edge_weight\n",
        "            if new_distance < table.loc[neighbor, 'Distance']:\n",
        "                update_table(neighbor, new_distance, current_node)\n",
        "                distance_update[neighbor] = f\"{current_distance}+{edge_weight}={new_distance}\"\n",
        "            else:\n",
        "                distance_update[neighbor] = f\"{table.loc[neighbor, 'Distance']}\"\n",
        "    updated_node_distances = dict(table['Distance'])\n",
        "    steps_table.loc[step] = [step, current_node, current_distance, neighbors, distance_update, updated_node_distances, unvisited_nodes.copy()]\n",
        "    unvisited_nodes.remove(current_node)\n",
        "    if not unvisited_nodes:\n",
        "        break\n",
        "    current_node = table.loc[list(unvisited_nodes), 'Distance'].idxmin()\n",
        "    step += 1\n",
        "\n",
        "# Validate the results using NetworkX's built-in Dijkstra's algorithm\n",
        "# NetworkX implementation of Dijkstra's algorithm for validation\n",
        "nx_shortest_path = nx.dijkstra_path(G_2, source=source_node, target=target_node)\n",
        "nx_shortest_path_length = nx.dijkstra_path_length(G_2, source=source_node, target=target_node)\n",
        "\n",
        "# Validate the shortest path and its length\n",
        "validation_result = (nx_shortest_path, nx_shortest_path_length) == (shortest_path, shortest_path_length)\n",
        "validation_result, nx_shortest_path, nx_shortest_path_length, shortest_path, shortest_path_length\n",
        "\n",
        "# Save table in Excel file\n",
        "steps_table.to_excel('steps_table.xlsx', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 10. Comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 10.1 Simulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to run Monte Carlo simulation for a single start entry\n",
        "def run_simulation_for_start(policy_final, shortest_path, start_entry, num_runs):\n",
        "    policy_results = []\n",
        "    shortest_path_results = []\n",
        "\n",
        "    for _ in range(num_runs):\n",
        "        starting_node = start_entry['starting_node']\n",
        "        elapsed_time = start_entry['elapsed_time']\n",
        "\n",
        "        # Simulate travel using the optimal policy\n",
        "        current_node = starting_node\n",
        "        total_time_policy = elapsed_time\n",
        "        path_policy = [current_node]\n",
        "\n",
        "        while current_node != final_node:\n",
        "            next_node = policy_final[(current_node, total_time_policy)][\"best_next_node\"]\n",
        "            p_planned, p_delayed, t_planned, t_delayed = get_transition_values(current_node, next_node)\n",
        "            \n",
        "            # Simulate whether the travel is planned or delayed\n",
        "            travel_time = np.random.choice([t_planned, t_delayed], p=[p_planned, p_delayed])\n",
        "            \n",
        "            total_time_policy += travel_time\n",
        "            total_time_policy = round(total_time_policy, 4)\n",
        "            current_node = next_node\n",
        "            path_policy.append(current_node)\n",
        "        \n",
        "        realized_deprivation_level_policy = deprivation_level_function(total_time_policy)\n",
        "        policy_results.append({\n",
        "            \"realized_deprivation_level\": realized_deprivation_level_policy,\n",
        "            \"total_time\": total_time_policy,\n",
        "            \"elapsed_time_node6\": str(elapsed_time),\n",
        "        })\n",
        "\n",
        "        # Simulate travel using the shortest path\n",
        "        total_time_shortest_path = elapsed_time\n",
        "        for i in range(shortest_path.index(starting_node), len(shortest_path) - 1):\n",
        "            from_node = shortest_path[i]\n",
        "            to_node = shortest_path[i + 1]\n",
        "            p_planned, p_delayed, t_planned, t_delayed = get_transition_values(from_node, to_node)\n",
        "            \n",
        "            # Simulate whether the travel is planned or delayed\n",
        "            travel_time = np.random.choice([t_planned, t_delayed], p=[p_planned, p_delayed])\n",
        "            total_time_shortest_path += travel_time\n",
        "    \n",
        "        # Calculate the realized deprivation level\n",
        "        realized_deprivation_level_shortest_path = deprivation_level_function(total_time_shortest_path)\n",
        "        shortest_path_results.append({\n",
        "            \"realized_deprivation_level\": realized_deprivation_level_shortest_path,\n",
        "            \"total_time\": total_time_shortest_path,\n",
        "            \"elapsed_time_node6\": str(elapsed_time),\n",
        "        })\n",
        "    \n",
        "    return policy_results, shortest_path_results\n",
        "\n",
        "# Function to run Monte Carlo simulation for all start entries\n",
        "def monte_carlo_simulation(policy_final, shortest_path, start_data_list, num_runs):\n",
        "    all_policy_results = []\n",
        "    all_shortest_path_results = []\n",
        "\n",
        "    for start_entry in start_data_list:\n",
        "        policy_results, shortest_path_results = run_simulation_for_start(policy_final, shortest_path, start_entry, num_runs)\n",
        "        all_policy_results.extend(policy_results)\n",
        "        all_shortest_path_results.extend(shortest_path_results)\n",
        "    \n",
        "    return all_policy_results, all_shortest_path_results\n",
        "\n",
        "start_data_list = [\n",
        "    {'starting_node': 6, 'elapsed_time': 2.1},\n",
        "    {'starting_node': 6, 'elapsed_time': 2.0},\n",
        "    {'starting_node': 6, 'elapsed_time': 2.6},\n",
        "    {'starting_node': 6, 'elapsed_time': 1.8},\n",
        "    {'starting_node': 6, 'elapsed_time': 2.3},\n",
        "]\n",
        "\n",
        "# Run the Monte Carlo simulation\n",
        "num_runs = 1000\n",
        "policy_results, shortest_path_results = monte_carlo_simulation(policy_final, shortest_path, start_data_list, num_runs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 10.2 Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert results to DataFrame for analysis\n",
        "policy_results_df = pd.DataFrame(policy_results, columns=['realized_deprivation_level', 'total_time', 'elapsed_time_node6'])\n",
        "shortest_path_results_df = pd.DataFrame(shortest_path_results, columns=['realized_deprivation_level', 'total_time', 'elapsed_time_node6'])\n",
        "\n",
        "# Statistical analysis\n",
        "policy_mean_deprivation = policy_results_df['realized_deprivation_level'].mean()\n",
        "shortest_mean_deprivation = shortest_path_results_df['realized_deprivation_level'].mean()\n",
        "\n",
        "policy_mean_time = policy_results_df['total_time'].mean()\n",
        "shortest_mean_time = shortest_path_results_df['total_time'].mean()\n",
        "\n",
        "print(f\"Policy Mean Deprivation: {policy_mean_deprivation}\")\n",
        "print(f\"Shortest Path Mean Deprivation: {shortest_mean_deprivation}\")\n",
        "print(f\"Policy Mean Travel Time: {policy_mean_time}\")\n",
        "print(f\"Shortest Path Mean Travel Time: {shortest_mean_time}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Combined Visualization\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "# First subplot\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(policy_results_df['realized_deprivation_level'], bins=80, alpha=0.7, label='Policy', color='#012D63', edgecolor='black')\n",
        "plt.xlabel('Realized Deprivation Level')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend()\n",
        "\n",
        "# Second subplot\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(shortest_path_results_df['realized_deprivation_level'], bins=80, alpha=0.7, label='Shortest Path', color='grey', edgecolor='black')\n",
        "plt.xlabel('Realized Deprivation Level')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('MCS_results.pdf', format='pdf', bbox_inches='tight', dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 10.3 Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "policy_results_df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "shortest_path_results_df.describe()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "new_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
