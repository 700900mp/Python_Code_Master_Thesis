{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# A Stochastic Dynamic Programming Model for Post-Disaster Relief Delivery considering the Deprivation Level"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 0. Install packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Library for processing, analyzing, and displaying data\n",
        "import pandas as pd\n",
        "\n",
        "# Library for dictionary subclass that remembers the order in which keys were first inserted\n",
        "from collections import OrderedDict\n",
        "\n",
        "# Library for the creation, manipulation, and study of complex networks of nodes and edges\n",
        "import networkx as nx\n",
        "\n",
        "# Library used for creating static, interactive, and animated visualizations\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Library for calculus\n",
        "import numpy as np\n",
        "\n",
        "# Library for finding a solution to the system of equations\n",
        "from scipy.optimize import fsolve\n",
        "\n",
        "# Library for data visualization\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Read arc data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_data(file_path: str):\n",
        "    df = pd.read_excel(file_path)\n",
        "    df = df.astype({\n",
        "        \"from_node\": int,\n",
        "        \"to_node\": int,\n",
        "        \"t_1\": float,\n",
        "        \"t_2\": float,\n",
        "        \"t_3\": float,\n",
        "        \"t_4\": float,\n",
        "        \"t_5\": float,\n",
        "        \"p_1\": float,\n",
        "        \"p_2\": float,\n",
        "        \"p_3\": float,\n",
        "        \"p_4\": float,\n",
        "        \"p_5\": float\n",
        "    })\n",
        "    return df\n",
        "\n",
        "file_path = \"/Users/minhthipham/Library/CloudStorage/OneDrive-ErasmusUniversityRotterdam/Thesis/04_Simulation network/Arc data_v3.xlsx\"\n",
        "df = load_data(file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Create a graph and map paths to arcs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a directed graph\n",
        "G = nx.DiGraph()\n",
        "\n",
        "# Add edges (arcs) to the graph\n",
        "G.add_edges_from([(0, 1), (0, 2), (0, 3),\n",
        "                  (1, 4), (1, 5), (1, 6),\n",
        "                  (2, 4), (2, 5), (2, 6),\n",
        "                  (3, 4), (3, 5), (3, 6),\n",
        "                  (4, 7), (4, 8), (4, 9),\n",
        "                  (5, 7), (5, 8), (5, 9),\n",
        "                  (6, 7), (6, 8), (6, 9),\n",
        "                  (7, 10), (7, 11), (7, 12),\n",
        "                  (8, 10), (8, 11), (8, 12),\n",
        "                  (9, 10), (9, 11), (9, 12),\n",
        "                  (10, 13), (10, 14),\n",
        "                  (11, 13), (11, 14),\n",
        "                  (12, 13), (12, 14),\n",
        "                  (13, 15),\n",
        "                  (14, 15)])\n",
        "\n",
        "# Define node positions\n",
        "positions = {0: (0, 1), 1: (1, 2), 2: (1, 1), 3: (1, 0),\n",
        "       4: (2, 2), 5: (2, 1), 6: (2, 0),\n",
        "       7: (3, 2), 8: (3, 1), 9: (3, 0),\n",
        "       10: (4, 2), 11: (4, 1), 12: (4, 0),\n",
        "       13: (5, 1.5), 14: (5, 0.5),\n",
        "       15: (6, 1)}\n",
        "\n",
        "# Plot the graph\n",
        "nx.draw(G, positions, with_labels=True, node_size=1500, node_color='lightgrey', font_weight='bold', arrowsize=20)\n",
        "plt.show()\n",
        "\n",
        "# Find all paths from source to target\n",
        "all_paths = list(nx.all_simple_paths(G, source=0, target=max(G.nodes)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Nodes and their descendants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reverse the graph to calculate predecessors\n",
        "G_reversed = G.reverse()\n",
        "descendants_dict = OrderedDict((node, list(G_reversed.predecessors(node))) for node in G_reversed.nodes())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. All possible states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Precompute deprivation levels for a relevant range of times\n",
        "unique_times = set()\n",
        "def calculate_states(df):\n",
        "    for _, row in df.iterrows():\n",
        "        from_node = int(row['from_node'])\n",
        "        to_node = int(row['to_node'])\n",
        "        t_1 = row['t_1']\n",
        "        t_2 = row['t_2']\n",
        "        t_3 = row['t_3']\n",
        "        t_4 = row['t_4']\n",
        "        t_5 = row['t_5']\n",
        "\n",
        "        if from_node in states_dict:\n",
        "            if to_node not in states_dict:\n",
        "                states_dict[to_node] = []\n",
        "            for state in states_dict[from_node]:\n",
        "                current_time = state[1]\n",
        "                new_time_1 = round(current_time + t_1, 4)\n",
        "                new_time_2 = round(current_time + t_2, 4)\n",
        "                new_time_3 = round(current_time + t_3, 4)\n",
        "                new_time_4 = round(current_time + t_4, 4)\n",
        "                new_time_5 = round(current_time + t_5, 4)\n",
        "                states_dict[to_node].append((to_node, new_time_1))\n",
        "                states_dict[to_node].append((to_node, new_time_2))\n",
        "                states_dict[to_node].append((to_node, new_time_3))\n",
        "                states_dict[to_node].append((to_node, new_time_4))\n",
        "                states_dict[to_node].append((to_node, new_time_5))\n",
        "                unique_times.update([new_time_1, new_time_2, new_time_3, new_time_4, new_time_5])\n",
        "\n",
        "states_dict = OrderedDict([(0, [(0, 0)])]) # Initialize states_dict\n",
        "calculate_states(df)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. Define Deprivation Level Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the deprivation level function\n",
        "def deprivation_level_function(x):\n",
        "    return 9.772697 / (1 + 3.9031 * np.exp(-0.7919 * x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6. Inflection point"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the original function and its derivatives\n",
        "def Y(X):\n",
        "    return 9.772697 / (1 + 3.9031 * np.exp(-0.7919 * X))\n",
        "\n",
        "def dY_dX(X):\n",
        "    A = 9.772697\n",
        "    B = 3.9031\n",
        "    k = 0.7919\n",
        "    return (A * k * B * np.exp(-k * X)) / ((1 + B * np.exp(-k * X)) ** 2)\n",
        "\n",
        "def d2Y_dX2(X):\n",
        "    A = 9.772697\n",
        "    B = 3.9031\n",
        "    k = 0.7919\n",
        "    term1 = (-A * k**2 * B * np.exp(-k * X)) / ((1 + B * np.exp(-k * X)) ** 2)\n",
        "    term2 = (2 * A * k**2 * B**2 * np.exp(-2 * k * X)) / ((1 + B * np.exp(-k * X)) ** 3)\n",
        "    return term1 + term2\n",
        "\n",
        "# Generate X values\n",
        "X_values = np.linspace(0, 10, 400)\n",
        "\n",
        "# Calculate Y, first derivative, and second derivative values\n",
        "Y_values = Y(X_values)\n",
        "dY_dX_values = dY_dX(X_values)\n",
        "d2Y_dX2_values = d2Y_dX2(X_values)\n",
        "\n",
        "# Find the X value where the first derivative has its maximum\n",
        "max_dY_dX_index = np.argmax(dY_dX_values)\n",
        "max_dY_dX_X_value = X_values[max_dY_dX_index]\n",
        "\n",
        "# Find the inflection point (where second derivative equals zero)\n",
        "inflection_point = fsolve(d2Y_dX2, 2)[0]\n",
        "\n",
        "# Set font to Times New Roman\n",
        "plt.rcParams['font.family'] = 'Times New Roman'\n",
        "\n",
        "# Plot the original function\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "plt.subplot(3, 1, 1)\n",
        "plt.plot(X_values, Y_values, label='Y(X)', color='black')\n",
        "plt.axvline(x=inflection_point, color='#5A6F97', linestyle='--', label='Inflection Point')\n",
        "plt.title('Deprivation Level over Time')\n",
        "plt.xlabel('Deprivation Time (days)')\n",
        "plt.ylabel('Deprivation Level')\n",
        "plt.xlim(0, 8)\n",
        "plt.grid(True, which='both', linewidth=0.5, color='lightgrey')\n",
        "plt.legend()\n",
        "\n",
        "# Plot the first derivative\n",
        "plt.subplot(3, 1, 2)\n",
        "plt.plot(X_values, dY_dX_values, label=\"dY/dX\", color='black')\n",
        "plt.axvline(x=inflection_point, color='#5A6F97', linestyle='--', label='Inflection Point')\n",
        "plt.title('Rate of Change of Deprivation Level')\n",
        "plt.xlabel('Deprivation Time (days)')\n",
        "plt.ylabel('dY/dX')\n",
        "plt.xlim(0, 8)\n",
        "plt.grid(True, which='both', linewidth=0.5, color='lightgrey')\n",
        "plt.legend()\n",
        "\n",
        "# Plot the second derivative\n",
        "plt.subplot(3, 1, 3)\n",
        "plt.plot(X_values, d2Y_dX2_values, label=\"d2Y/dX2\", color='black')\n",
        "plt.axvline(x=inflection_point, color='#5A6F97', linestyle='--', label='Inflection Point')\n",
        "plt.title('Acceleration of Change of Deprivation Level')\n",
        "plt.xlabel('Deprivation Time (days)')\n",
        "plt.ylabel('d2Y/dX2')\n",
        "plt.xlim(0, 8)\n",
        "plt.grid(True, which='both', linewidth=0.5, color='lightgrey')\n",
        "plt.legend()\n",
        "\n",
        "# Adjust the space between plots\n",
        "plt.subplots_adjust(hspace=0.5)  # Adjust the hspace value for better spacing\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('Derivatives_DLF.pdf', format='pdf', bbox_inches='tight', dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7. Recursion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper function to get transition probabilities and times\n",
        "def get_transition_values(n, d):\n",
        "    row = df.loc[(df[\"from_node\"] == n) & (df[\"to_node\"] == d)]\n",
        "    p_1 = row[\"p_1\"].values[0]\n",
        "    p_2 = row[\"p_2\"].values[0]\n",
        "    p_3 = row[\"p_3\"].values[0]\n",
        "    p_4 = row[\"p_4\"].values[0]\n",
        "    p_5 = row[\"p_5\"].values[0]\n",
        "    t_1 = row[\"t_1\"].values[0]\n",
        "    t_2 = row[\"t_2\"].values[0]\n",
        "    t_3 = row[\"t_3\"].values[0]\n",
        "    t_4 = row[\"t_4\"].values[0]\n",
        "    t_5 = row[\"t_5\"].values[0]\n",
        "    return p_1, p_2, p_3, p_4, p_5, t_1, t_2, t_3, t_4, t_5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Determine the optimal policy\n",
        "policy = {}\n",
        "v_dict = OrderedDict()\n",
        "v_dict_all = OrderedDict()\n",
        "final_node = max(G.nodes)\n",
        "\n",
        "for n, states in reversed(states_dict.items()):\n",
        "    for state in states:\n",
        "        tau = state[1]\n",
        "        if n == final_node:\n",
        "            v_dict[state] = deprivation_level_function(tau)\n",
        "        else:\n",
        "            min_V = np.inf\n",
        "            best_d = None\n",
        "            for d in descendants_dict.get(n, []):\n",
        "                p_1, p_2, p_3, p_4, p_5, t_1, t_2, t_3, t_4, t_5 = get_transition_values(n, d)\n",
        "                v_d_1 = v_dict.get((d, round(tau + t_1, 4)))\n",
        "                v_d_2 = v_dict.get((d, round(tau + t_2, 4)))\n",
        "                v_d_3 = v_dict.get((d, round(tau + t_3, 4)))\n",
        "                v_d_4 = v_dict.get((d, round(tau + t_4, 4)))\n",
        "                v_d_5 = v_dict.get((d, round(tau + t_5, 4)))\n",
        "                V = p_1 * v_d_1 + p_2 * v_d_2 + p_3 * v_d_3 + p_4 * v_d_4 + p_5 * v_d_5 \n",
        "                v_dict_all[(n, tau, d)] = V\n",
        "                if V < min_V:\n",
        "                    min_V = V\n",
        "                    best_d = d\n",
        "            # cache deprivation levels\n",
        "            v_dict[state] = min_V\n",
        "            if best_d is not None:\n",
        "                policy[state] = {\n",
        "                    \"best_next_node\": best_d, \n",
        "                    \"total_expected_deprivation_level\": min_V\n",
        "                }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8. Final policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import copy\n",
        "\n",
        "# Determine the optimal policy\n",
        "policy_final = copy.deepcopy(policy)\n",
        "for state, _ in policy_final.items():\n",
        "    n = state[0]\n",
        "    tau = state[1]\n",
        "    min_t_1 = np.inf\n",
        "    if tau >= inflection_point:\n",
        "        for d in descendants_dict.get(n, []):\n",
        "            p_1, _, _, _, _, t_1, _, _, _, _ = get_transition_values(n, d)\n",
        "            if t_1 < min_t_1:\n",
        "                min_t_1 = t_1\n",
        "                min_p_1 = p_1\n",
        "                best_d = d\n",
        "            elif t_1 == min_t_1 and p_1 > min_p_1: # if any descendants have the same t_1 choose the one with the higher probability\n",
        "                min_t_1 = t_1\n",
        "                min_p_1 = p_1\n",
        "                best_d = d\n",
        "        policy_final[state][\"best_next_node\"] = best_d\n",
        "        policy_final[state][\"total_expected_deprivation_level\"] = v_dict_all[n, tau, best_d]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "policy_df = pd.DataFrame.from_dict(data=policy_final, orient='index')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Transforming the dictionary into a DataFrame\n",
        "data = []\n",
        "for key, value in policy_final.items():\n",
        "    row = {\n",
        "        'state': key,\n",
        "        'best_next_node': value['best_next_node'],\n",
        "        'total_expected_deprivation_level': value['total_expected_deprivation_level']\n",
        "    }\n",
        "    data.append(row)\n",
        "\n",
        "df_out = pd.DataFrame(data)\n",
        "\n",
        "# Save the DataFrame to an Excel file\n",
        "df_out.to_excel('output_v3.xlsx', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9. Expected Shortest Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a directed graph\n",
        "G_2 = nx.DiGraph()\n",
        "\n",
        "# Add edges with expected travel time as weights\n",
        "for _, row in df.iterrows():\n",
        "    expected_travel_time = round(row['t_1'] * row['p_1'] + row['t_2'] * row['p_2'] + row['t_3'] * row['p_3'] + row['t_4'] * row['p_4'] + row['t_5'] * row['p_5'], 4)\n",
        "    G_2.add_edge(row['from_node'], row['to_node'], weight=expected_travel_time)\n",
        "\n",
        "# Compute the shortest path based on expected travel time\n",
        "source = 0\n",
        "target = max(G_2.nodes)\n",
        "shortest_path = nx.shortest_path(G_2, source=source, target=target, weight='weight')\n",
        "shortest_path_length = nx.shortest_path_length(G_2, source=source, target=target, weight='weight')\n",
        "shortest_path_DL = deprivation_level_function(shortest_path_length)\n",
        "\n",
        "# Display results\n",
        "print(f\"The shortest path from node {source} to node {target} is: {shortest_path}\")\n",
        "print(f\"The total expected travel time is: {shortest_path_length} days\")\n",
        "print(f\"The total expected deprivation level is: {shortest_path_DL}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a directed graph\n",
        "G_2 = nx.DiGraph()\n",
        "\n",
        "# Add edges with expected travel time as weights\n",
        "for _, row in df.iterrows():\n",
        "    expected_travel_time = round(row['t_1'] * row['p_1'] + row['t_2'] * row['p_2'] + row['t_3'] * row['p_3'] + row['t_4'] * row['p_4'] + row['t_5'] * row['p_5'], 4)\n",
        "    G_2.add_edge(row['from_node'], row['to_node'], weight=expected_travel_time)\n",
        "\n",
        "# Define source and target nodes for the shortest path calculation\n",
        "source_node = 0\n",
        "target_node = max(G_2.nodes)\n",
        "\n",
        "# Apply Dijkstra's algorithm to find the shortest path and its length\n",
        "shortest_path = nx.dijkstra_path(G_2, source=source_node, target=target_node)\n",
        "shortest_path_length = nx.dijkstra_path_length(G_2, source=source_node, target=target_node)\n",
        "\n",
        "# Initialize the table with infinity distances\n",
        "nodes = list(G_2.nodes)\n",
        "table = pd.DataFrame(index=nodes, columns=['Distance', 'Previous Node'])\n",
        "table['Distance'] = float('inf')\n",
        "table['Previous Node'] = None\n",
        "\n",
        "# Function to update the table\n",
        "def update_table(node, distance, previous_node):\n",
        "    table.at[node, 'Distance'] = distance\n",
        "    table.at[node, 'Previous Node'] = previous_node\n",
        "\n",
        "# Initialize the source node\n",
        "update_table(source_node, 0, '-')\n",
        "\n",
        "# Dijkstra's algorithm steps\n",
        "steps_table_columns = ['Step', 'Current Node', 'Distance from Source', 'Neighbors', 'Distance Update', 'Updated Node Distances', 'Unvisited Nodes']\n",
        "steps_table = pd.DataFrame(columns=steps_table_columns)\n",
        "\n",
        "unvisited_nodes = set(nodes)\n",
        "current_node = source_node\n",
        "step = 1\n",
        "\n",
        "while unvisited_nodes:\n",
        "    current_distance = table.loc[current_node, 'Distance']\n",
        "    neighbors = list(G_2.neighbors(current_node))\n",
        "    distance_update = {}\n",
        "    for neighbor in neighbors:\n",
        "        if neighbor in unvisited_nodes:\n",
        "            edge_weight = G_2[current_node][neighbor]['weight']\n",
        "            new_distance = current_distance + edge_weight\n",
        "            if new_distance < table.loc[neighbor, 'Distance']:\n",
        "                update_table(neighbor, new_distance, current_node)\n",
        "                distance_update[neighbor] = f\"{current_distance}+{edge_weight}={new_distance}\"\n",
        "            else:\n",
        "                distance_update[neighbor] = f\"{table.loc[neighbor, 'Distance']}\"\n",
        "    updated_node_distances = dict(table['Distance'])\n",
        "    steps_table.loc[step] = [step, current_node, current_distance, neighbors, distance_update, updated_node_distances, unvisited_nodes.copy()]\n",
        "    unvisited_nodes.remove(current_node)\n",
        "    if not unvisited_nodes:\n",
        "        break\n",
        "    current_node = table.loc[list(unvisited_nodes), 'Distance'].idxmin()\n",
        "    step += 1\n",
        "\n",
        "# Validate the results using NetworkX's built-in Dijkstra's algorithm\n",
        "# NetworkX implementation of Dijkstra's algorithm for validation\n",
        "nx_shortest_path = nx.dijkstra_path(G_2, source=source_node, target=target_node)\n",
        "nx_shortest_path_length = nx.dijkstra_path_length(G_2, source=source_node, target=target_node)\n",
        "\n",
        "# Validate the shortest path and its length\n",
        "validation_result = (nx_shortest_path, nx_shortest_path_length) == (shortest_path, shortest_path_length)\n",
        "validation_result, nx_shortest_path, nx_shortest_path_length, shortest_path, shortest_path_length\n",
        "\n",
        "# Save table in Excel file\n",
        "steps_table.to_excel('steps_table_v3.xlsx', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 10. Comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 10.1 Get all unique paths in policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to find all unique paths starting from a given node\n",
        "def find_unique_paths(df, start_node, end_node):\n",
        "    paths = set()\n",
        "    \n",
        "    def traverse_path(node, path):\n",
        "        path = path + [node]\n",
        "        \n",
        "        if node == end_node:\n",
        "            paths.add(tuple(path))  # Convert list to tuple to ensure uniqueness in a set\n",
        "            return\n",
        "        \n",
        "        # Find the next node based on the current node\n",
        "        next_nodes = df.loc[(node, slice(None)), 'best_next_node'].tolist()\n",
        "        \n",
        "        for next_node in next_nodes:\n",
        "            traverse_path(next_node, path)\n",
        "    \n",
        "    traverse_path(start_node, [])\n",
        "    return [list(p) for p in paths]  # Convert each path back to a list\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 10.2 Simulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to run Monte Carlo simulation for a single start entry\n",
        "def run_simulation_for_start(policy_final, shortest_path, start_entry, num_runs):\n",
        "    policy_results = []\n",
        "    shortest_path_results = []\n",
        "\n",
        "    for _ in range(num_runs):\n",
        "        starting_node = start_entry['starting_node']\n",
        "        elapsed_time = start_entry['elapsed_time']\n",
        "\n",
        "        # Simulate travel using the optimal policy\n",
        "        current_node = starting_node\n",
        "        total_time_policy = elapsed_time\n",
        "        path_policy = [current_node]\n",
        "\n",
        "        while current_node != final_node:\n",
        "            next_node = policy_final[(current_node, total_time_policy)][\"best_next_node\"]\n",
        "            p_1, p_2, p_3, p_4, p_5, t_1, t_2, t_3, t_4, t_5 = get_transition_values(current_node, next_node)\n",
        "            \n",
        "            # Simulate the travel scenario\n",
        "            travel_time = np.random.choice([t_1, t_2, t_3, t_4, t_5], p=[p_1, p_2, p_3, p_4, p_5])\n",
        "            \n",
        "            total_time_policy += travel_time\n",
        "            total_time_policy = round(total_time_policy, 4)\n",
        "            current_node = next_node\n",
        "            path_policy.append(current_node)\n",
        "        \n",
        "        realized_deprivation_level_policy = deprivation_level_function(total_time_policy)\n",
        "        policy_results.append({\n",
        "            \"realized_deprivation_level\": realized_deprivation_level_policy,\n",
        "            \"total_time\": total_time_policy,\n",
        "            \"elapsed_time_node\": str(elapsed_time),\n",
        "        })\n",
        "\n",
        "        # Simulate travel using the shortest path\n",
        "        total_time_shortest_path = elapsed_time\n",
        "        for i in range(shortest_path.index(starting_node), len(shortest_path) - 1):\n",
        "            from_node = shortest_path[i]\n",
        "            to_node = shortest_path[i + 1]\n",
        "            p_1, p_2, p_3, p_4, p_5, t_1, t_2, t_3, t_4, t_5 = get_transition_values(from_node, to_node)\n",
        "            \n",
        "            # Simulate the travel time scenario\n",
        "            travel_time = np.random.choice([t_1, t_2, t_3, t_4, t_5], p=[p_1, p_2, p_3, p_4, p_5])\n",
        "            total_time_shortest_path += travel_time\n",
        "    \n",
        "        # Calculate the realized deprivation level\n",
        "        realized_deprivation_level_shortest_path = deprivation_level_function(total_time_shortest_path)\n",
        "        shortest_path_results.append({\n",
        "            \"realized_deprivation_level\": realized_deprivation_level_shortest_path,\n",
        "            \"total_time\": total_time_shortest_path,\n",
        "            \"elapsed_time_node\": str(elapsed_time),\n",
        "        })\n",
        "    \n",
        "    return policy_results, shortest_path_results\n",
        "\n",
        "# Function to run Monte Carlo simulation for all start entries\n",
        "def monte_carlo_simulation(policy_final, shortest_path, start_data_list, num_runs):\n",
        "    all_policy_results = []\n",
        "    all_shortest_path_results = []\n",
        "\n",
        "    for start_entry in start_data_list:\n",
        "        policy_results, shortest_path_results = run_simulation_for_start(policy_final, shortest_path, start_entry, num_runs)\n",
        "        all_policy_results.extend(policy_results)\n",
        "        all_shortest_path_results.extend(shortest_path_results)\n",
        "    \n",
        "    return all_policy_results, all_shortest_path_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_runs = 1000\n",
        "\n",
        "start_data_list_1 = [\n",
        "    {'starting_node': 12, 'elapsed_time': 1.72},\n",
        "    {'starting_node': 12, 'elapsed_time': 1.73},\n",
        "    {'starting_node': 12, 'elapsed_time': 1.74},\n",
        "    {'starting_node': 12, 'elapsed_time': 1.75},\n",
        "    {'starting_node': 12, 'elapsed_time': 1.76},\n",
        "    {'starting_node': 12, 'elapsed_time': 1.77},\n",
        "    {'starting_node': 12, 'elapsed_time': 1.78},\n",
        "    {'starting_node': 12, 'elapsed_time': 1.79},\n",
        "    {'starting_node': 12, 'elapsed_time': 1.8},\n",
        "    {'starting_node': 12, 'elapsed_time': 1.81},\n",
        "    {'starting_node': 12, 'elapsed_time': 1.82},\n",
        "    {'starting_node': 12, 'elapsed_time': 1.83},\n",
        "    {'starting_node': 12, 'elapsed_time': 1.84},\n",
        "    {'starting_node': 12, 'elapsed_time': 1.85},\n",
        "    {'starting_node': 12, 'elapsed_time': 1.86},\n",
        "    {'starting_node': 12, 'elapsed_time': 1.87},\n",
        "    {'starting_node': 12, 'elapsed_time': 1.88},\n",
        "    {'starting_node': 12, 'elapsed_time': 1.89},\n",
        "    {'starting_node': 12, 'elapsed_time': 1.9},\n",
        "    {'starting_node': 12, 'elapsed_time': 1.91},\n",
        "    {'starting_node': 12, 'elapsed_time': 1.92},\n",
        "    {'starting_node': 12, 'elapsed_time': 1.93},\n",
        "    {'starting_node': 12, 'elapsed_time': 1.94},\n",
        "    {'starting_node': 12, 'elapsed_time': 1.95},\n",
        "    {'starting_node': 12, 'elapsed_time': 1.96},\n",
        "    {'starting_node': 12, 'elapsed_time': 1.97},\n",
        "    {'starting_node': 12, 'elapsed_time': 1.98},\n",
        "    {'starting_node': 12, 'elapsed_time': 1.99},\n",
        "    {'starting_node': 12, 'elapsed_time': 2.0},\n",
        "    {'starting_node': 12, 'elapsed_time': 2.01},\n",
        "    {'starting_node': 12, 'elapsed_time': 2.02},\n",
        "    {'starting_node': 12, 'elapsed_time': 2.03},\n",
        "    {'starting_node': 12, 'elapsed_time': 2.04},\n",
        "    {'starting_node': 12, 'elapsed_time': 2.05},\n",
        "    {'starting_node': 12, 'elapsed_time': 2.06},\n",
        "    {'starting_node': 12, 'elapsed_time': 2.07},\n",
        "    {'starting_node': 12, 'elapsed_time': 2.08},\n",
        "    {'starting_node': 12, 'elapsed_time': 2.09},\n",
        "    {'starting_node': 12, 'elapsed_time': 2.1},\n",
        "    {'starting_node': 12, 'elapsed_time': 2.11},\n",
        "    {'starting_node': 12, 'elapsed_time': 2.12},\n",
        "    {'starting_node': 12, 'elapsed_time': 2.13},\n",
        "    {'starting_node': 12, 'elapsed_time': 2.14},\n",
        "    {'starting_node': 12, 'elapsed_time': 2.15},\n",
        "    {'starting_node': 12, 'elapsed_time': 2.16},\n",
        "    {'starting_node': 12, 'elapsed_time': 2.17},\n",
        "    {'starting_node': 12, 'elapsed_time': 2.18},\n",
        "    {'starting_node': 12, 'elapsed_time': 2.19},\n",
        "    {'starting_node': 12, 'elapsed_time': 2.2},\n",
        "    {'starting_node': 12, 'elapsed_time': 2.21},\n",
        "    {'starting_node': 12, 'elapsed_time': 2.22},\n",
        "    {'starting_node': 12, 'elapsed_time': 2.23},\n",
        "    {'starting_node': 12, 'elapsed_time': 2.24},\n",
        "    {'starting_node': 12, 'elapsed_time': 2.25},\n",
        "    {'starting_node': 12, 'elapsed_time': 2.26},\n",
        "    {'starting_node': 12, 'elapsed_time': 2.27},\n",
        "    {'starting_node': 12, 'elapsed_time': 2.28},\n",
        "    {'starting_node': 12, 'elapsed_time': 2.29},\n",
        "    {'starting_node': 12, 'elapsed_time': 2.3},\n",
        "    {'starting_node': 12, 'elapsed_time': 2.31},\n",
        "    {'starting_node': 12, 'elapsed_time': 2.32},\n",
        "    {'starting_node': 12, 'elapsed_time': 2.33},\n",
        "    {'starting_node': 12, 'elapsed_time': 2.34},\n",
        "    {'starting_node': 12, 'elapsed_time': 2.35},\n",
        "    {'starting_node': 12, 'elapsed_time': 2.36},\n",
        "    {'starting_node': 12, 'elapsed_time': 2.37},\n",
        "    {'starting_node': 12, 'elapsed_time': 2.38},\n",
        "    {'starting_node': 12, 'elapsed_time': 2.39},\n",
        "    {'starting_node': 12, 'elapsed_time': 2.4},\n",
        "    {'starting_node': 12, 'elapsed_time': 2.41},\n",
        "    {'starting_node': 12, 'elapsed_time': 2.42},\n",
        "    {'starting_node': 12, 'elapsed_time': 2.43},\n",
        "    {'starting_node': 12, 'elapsed_time': 2.44},\n",
        "    {'starting_node': 12, 'elapsed_time': 2.45},\n",
        "    {'starting_node': 12, 'elapsed_time': 2.46},\n",
        "    {'starting_node': 12, 'elapsed_time': 2.47},\n",
        "    {'starting_node': 12, 'elapsed_time': 2.48},\n",
        "    {'starting_node': 12, 'elapsed_time': 2.49},\n",
        "    {'starting_node': 12, 'elapsed_time': 2.5},\n",
        "    {'starting_node': 12, 'elapsed_time': 2.51},\n",
        "    {'starting_node': 12, 'elapsed_time': 2.52},\n",
        "    {'starting_node': 12, 'elapsed_time': 2.53},\n",
        "    {'starting_node': 12, 'elapsed_time': 2.54},\n",
        "    {'starting_node': 12, 'elapsed_time': 2.55},\n",
        "    {'starting_node': 12, 'elapsed_time': 2.56},\n",
        "    {'starting_node': 12, 'elapsed_time': 2.57},\n",
        "    {'starting_node': 12, 'elapsed_time': 2.58},\n",
        "    {'starting_node': 12, 'elapsed_time': 2.59},\n",
        "    {'starting_node': 12, 'elapsed_time': 2.6},\n",
        "    {'starting_node': 12, 'elapsed_time': 2.61},\n",
        "    {'starting_node': 12, 'elapsed_time': 2.62},\n",
        "    {'starting_node': 12, 'elapsed_time': 2.63},\n",
        "    {'starting_node': 12, 'elapsed_time': 2.64},\n",
        "    {'starting_node': 12, 'elapsed_time': 2.65},\n",
        "    {'starting_node': 12, 'elapsed_time': 2.66},\n",
        "    {'starting_node': 12, 'elapsed_time': 2.67},\n",
        "    {'starting_node': 12, 'elapsed_time': 2.68},\n",
        "    {'starting_node': 12, 'elapsed_time': 2.69},\n",
        "    {'starting_node': 12, 'elapsed_time': 2.71},\n",
        "    {'starting_node': 12, 'elapsed_time': 2.73},\n",
        "    {'starting_node': 12, 'elapsed_time': 2.78}\n",
        "\n",
        "]\n",
        "\n",
        "start_data_list_2 = [\n",
        "    {'starting_node': 8, 'elapsed_time': 1.72},\n",
        "    {'starting_node': 8, 'elapsed_time': 1.73},\n",
        "    {'starting_node': 8, 'elapsed_time': 1.75},\n",
        "    {'starting_node': 8, 'elapsed_time': 1.76},\n",
        "    {'starting_node': 8, 'elapsed_time': 1.78},\n",
        "    {'starting_node': 8, 'elapsed_time': 1.8},\n",
        "    {'starting_node': 8, 'elapsed_time': 1.82},\n",
        "    {'starting_node': 8, 'elapsed_time': 1.87}\n",
        "]\n",
        "\n",
        "policy_results_1, shortest_path_results_1 = monte_carlo_simulation(policy_final, shortest_path, start_data_list_1, num_runs)\n",
        "policy_results_2, shortest_path_results_2 = monte_carlo_simulation(policy_final, shortest_path, start_data_list_2, num_runs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def turn_results_to_dataframe(policy_results, shortest_path_results):\n",
        "    policy_results_df = pd.DataFrame(policy_results, columns=['realized_deprivation_level', 'total_time', 'elapsed_time_node6'])\n",
        "    shortest_path_results_df = pd.DataFrame(shortest_path_results, columns=['realized_deprivation_level', 'total_time', 'elapsed_time_node6'])\n",
        "    return policy_results_df, shortest_path_results_df\n",
        "\n",
        "policy_results_1_df, shortest_path_results_1_df = turn_results_to_dataframe(policy_results_1, shortest_path_results_1)\n",
        "policy_results_2_df, shortest_path_results_2_df = turn_results_to_dataframe(policy_results_2, shortest_path_results_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 10.2 Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_distribution(df1, df2, fn, title):\n",
        "    print(f\"###{title}\")   \n",
        "    # Combined Visualization\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    # First subplot\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.hist(df1['realized_deprivation_level'], bins=80, alpha=0.7, label='Policy', color='#012D63', edgecolor='black')\n",
        "    plt.xlabel('Realized Deprivation Level')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.legend()\n",
        "\n",
        "    # Second subplot\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.hist(df2['realized_deprivation_level'], bins=80, alpha=0.7, label='Shortest Path', color='grey', edgecolor='black')\n",
        "    plt.xlabel('Realized Deprivation Level')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{fn}.pdf\", format='pdf', bbox_inches='tight', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "plot_distribution(policy_results_1_df, shortest_path_results_1_df, \"MCS_results_v3.1\", \"Results_1\")\n",
        "plot_distribution(policy_results_2_df, shortest_path_results_2_df, \"MCS_results_v3.2\", \"Results_2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 10.3 Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "policy_results_1_df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "shortest_path_results_1_df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "policy_results_2_df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "shortest_path_results_2_df.describe()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "new_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
